[
{
	"uri": "//localhost:1313/",
	"title": "Getting Start with AWS Bedrock",
	"tags": [],
	"description": "",
	"content": "Getting Start with AWS Bedrock Learn to AWS Bedrock Amazon Bedrock can be your best ally for creating personalized content, automating customer support, analyzing vast datasets, and generating images without needing weeks of work or a specilized AI team. As technology rapidly evolves, Amazon Bedrock emerges as the groundbreaking tool that opens the doors to genAI for everyone.\nHow to transform your ideas into powerfull application with out being a machine expert? Where the possibilities are as limitless as your imagination? We will cover: What is Amazon Bedrock? Amazon Bedrock Features Amazon Bedrock Pricing Amazon Bedrock Use Cases "
},
{
	"uri": "//localhost:1313/1-introduce/",
	"title": "Overview AWS Bedrock",
	"tags": [],
	"description": "",
	"content": "What is Amazon Bedrock? Amazon Bedrock is a generative AI tool provide by AWS, it\u0026rsquo;s a service that provides access to leading foundation models (FMs) which are advanced AI models that are used for different tasks and applications and can be adapted for specific tasks.\nAmazon Bedrock Features Access to various FMs: it has library many pre-trained models which are ready to use and can be modified to meet the specific needs that you have. Simplified and managed experience for GenAI application: is a serverless service with no needs to manage infratructure components for models. It provides a single API endpoint with choose model which help to improve ingrations and operation. Integration with other AWS services: you can easly integrate it with other AWS sevices like: Amazon SageMaker for training, AWS lambda for serverless computing, and Amazon S3 for data storage. Model customization: the actual models can be customized with your own data, you can create a private copy of the model and start working on it. To pair the models with up to date information you an use Retrieval Augmented Genaration (RAG), a technique that combines information retrieval with text generation in order to have more accurate responses. Security: your data will be secured, won\u0026rsquo;t be visible and won\u0026rsquo;t be exposed in public enviroments. All your data will be confidential and AWS Bedrock won\u0026rsquo;t save it for own purposes. Amazon Bedrock Models Amazon Bedrock provides access to high-performation AI models, developed both by AWS, and leading AI provides. The modles include:\nAmazon Titan: A group of models developed by Amazon, designed for tasks like generation, language understanding, and more. Anthropic\u0026rsquo;s Claude: A model focuesed on high-quality text generation and natural language understanding. AI Labs\u0026rsquo; Jurassic-2: Powerful natural language processing model, ideal for text generation, content creation, and advance writing tasks. Stability AI: Offers image generation models which allows users to create images from textual descriptions. It\u0026rsquo;s useful for application in design, marketing, and more. Meta\u0026rsquo;s Mistral: is a powerful language model developed by Meta. It\u0026rsquo;s designed for a variety of natural language processing tasks, including text generation. comprehension, and more. Meta\u0026rsquo;s models are know for their advanced capabilities and efficiency in handing complex language tasks. Cohere\u0026rsquo;s Command R: Offers the Command R series of models, which are optimized for retrieval-augmented generation (RAG). These models are effective at combining large-scale language models with retrieval systems to improve the relevance and accuracy of generated content. They are used for tasks that require both deep understanding and the ability to pull in relevant information from external sources. Amazon Bedrock Pricing Amazon bedrock pricing is based on the service usage and depends of the pricing model, there 3 of this:\nOn-demand: you pay just for each operation performerd using the available models. The price depends on the number of input and output tokens processed for the chosen foundation model. The token includes the characters or text units that you entered in prompt. In image generation case, you will have to pay each generated image. Provisioned throughput: this applies for some models where you can purchase the package wich guarantees its availability and usafe for around 1 to 6 months. Model customiztion: When you want to customize a model, you will have to pay for the tokens and the model storage is charged per month. For model inference costs depend on the specific model you use and the number of tokens processed. The range can be from $0.00004 to $0.03 per 1,000 tokens. For model training vary based on the complexity of the model and amount of data, a range can be from $1 to $10 per hour of training.\nAmazon Bedrock Use Cases Creation of personalized content: you can generate new content such as blogs, articles stories, descriptions, social media posts, among others, improving personalization and marketing features. Custom support automation: helps you to improve customer experience and supports, creating chatbots and virtual assistants trained with the correct information and knowledge, those can efficiently address customers requests, guides and provide solutions. Data analysis and insights generation: you can analyze big volumes of datasets and generate appropiate insights, this is value always when you want to understand patterns and drive better decisions. Personaliztion: This more valuable in e-commerce industries when you can recommend products to user based on its perferences, behaviors, browsing and search history so you/ll improve in sales and customer satisfaction. Text summarization: you can increase productivity by generating summaries of books, stories and different documentations, this is value when you want to understand legal documents, educational or academic contents and also have smmaries about your meetings! Image generation: you can create an image about anything that you want. can be realistic or artistic, in different enviroments just by entering the description of the image you want in a prompt. Amazon Bedrock Examples These are some of the models examples that has Bedrock, you just have to enter whatever you need in the prompt section and it will give you the response.\nSummarization Text generation Code generation Image generation Information extraction Conclusion Amazon Bedrock represents a significant leap forward in making generative AI accessible and practical for businesses of all sizes. With its robust features, seamless integration with AWS services, and the ability to customize models to fit specific needs, Bedrock empowers developers to innovate without the steep learning curve traditionally associated with AI. Whether you\u0026rsquo;re looking to enhance customer experiences, streamline content creation, or gain deeper insights from your data, Amazon Bedrock offers, secure, and flexible platform to turm your AI-driven vsions into reality. As the landscape of AI continues to evolve, embracting tools like Bedrock could be the key to statying ahead in a competitive market.\n"
},
{
	"uri": "//localhost:1313/2-keyfeature/",
	"title": "Key Features of Amazon Bedrock",
	"tags": [],
	"description": "",
	"content": "Key features of Amazon Bedrock 1. Access to a range of leading foundation models (FMs)\nBedrock offers various FMs from prominent AI companies, such as Anthropic, AI21 Labs, Cohere, Meta, Mistral AI, Stability.ai, and Amazon\u0026rsquo;s own modles. Since different models are best suited for different tasks, Bedrock gives teams flexibility regarding model choice for different scenarios.\n2. Simplified and managed experience for GenAI applications\nAmazon Bedrock is a fully managed and serverless service, entirely abstracting the need to manage infrastructure components for your foundation models. It provides a single API access regardless of the chosen model, simplifying integrations, operations, and model version upgrades.\n3. Model customization and Retrieval Augmented Generation (RAG)\nThe actual value of the power of FMs comes when companies manage to privately and effectively customize and adapt these models with their own proprietary data. To customize these models, Bedrock offers fine-tuning features and creates a separate private copy of this model. To pair the models with recent and up-to-date information, companies leverage RAG, a technique to enhance a model’s context with proprietary data sources for more accurate and informed responses.\n4. Built-in security, privacy, and safety\nWith Bedrock, the data never leave your AWS environments and are encrypted in transit and at rest. Users can leverage their existing AWS security controls and services, such as KMS for encryption, IAM policies, CloudWatch for monitoring, CloudTrail for governance, and network design based on Amazon VPC.\nWhen a base model is fine-tuned, a private copy of that model is used, and the proprietary data aren’t used to improve the base model. Bedrock is in scope for common compliance standards, including ISO, SOC, CSA, STAR Level 2, is HIPAA eligible, and can be used in compliance with the GDPR.\nBedrock has released the Guardrails functionality that allows companies to enforce policies and safety for their model responses. Guardrails provide a layer of safeguards and policies to promote safe and responsible usage of FMs by blocking unsafe topics, avoiding harmful content, and redacting personally identifiable information.\n5. Leverage Agents for executing multi-step tasks\nThere are use cases where companies would like to automate processes and execute complex multi-step tasks based on the model’s response. With Agents, users can fast-track their prompt creation with customized instructions, orchestrate a sequence of actions, call the necessary APIs to fulfill the desired task, and monitor and trace the agent’s reasoning and orchestration of complex tasks.\nCourse Models 1. Application Components\nObjectives Topics In the module, you will learn how to do the following: The module is organized into the following topics: * Describe the components of a generative AI application * Overview of Genaretive AI Application Components * Work with embedding and vector databases. * Foundation Models and the FM interface * Customize a foundation model using Retrieval Augmented Generation (RAG) and model fine-tuning. * Working with Datasets and Embeddings * Explain the architecture of a typical generative AI application * Additional Application Components * RAG * Model Fine-Tuning * Securing Generative AI Applications * Generative AI Application Architecture 2. Foundation Models\nObjective Topics In this module, you will learn how to do the following: The modules is organized the following topics: * Identify the foundation models available with Amazon Bedrock. * Introduction to Amazon Bedrock Foundation Models * Describe how to control the inference parameters to tune desired output. * Using Amazon Bedrock FMs for Inference * Describe how to use APIs to invoke foundation models or create customization jobs. * Amazon Bedrock Methods * Identify monitoring and logging capabilities and tools for governance and audit requirements. * Data Protection and Auditability 3. Using LangChain\nObjective Topics In this module, you will learn how to do the following: The module is organized into the following topics: * Identify common challenges practitioners face when developing LLMs. * Optimizing LLM Performance * Describe the benefits of using LangChain for training LLMs. * Integrating AWS and LangChain * Integrate LangChain with LLMs, prompt templates, chains, chat models, text embeddings models, document loaders, retrievers, and agents in Amazon Bedrock. * Using Models with LangChain * Use LangChain agents to manage external resources. * Constructing Prompts * Structuring Documents with Indexes * Storing and Retrieving Data with Memory * Using Chains to Sequence Components * Managing External Resources with LangChain Agents 4. Architecture Patterns\nObjective Topics In this module, you will learn how to do the following: The module is organized into the following topics: * Describe the text generation and text summarization architecture patterns. * Introduction to Architecture Patterns * Explain how to use the question answering pattern for generative AI applications. * Text Generation and Text Summarization * Describe how the chatbot pattern is used to enhance the user experience. * Question Answering * Understand how FMs can be used to generate code * Chatbots * Work with LangChain and agents for Amazon Bedrock. * Code Generation * LangChain and Agents Amazon Bedrock "
},
{
	"uri": "//localhost:1313/3-foundationmodels/",
	"title": "Typical componets of a generative AI application",
	"tags": [],
	"description": "",
	"content": "\nA foundation model (FM) is in the center surrounded by components. These include frontend web application or mobile app, FM interface, machine learning (ML) environment, model training, enterprise datasets, vector database, text and image embeddings, and long-term memory store. Governance and security are integrated into all components.\nFoundation models interface At the heart of a generative AI application is the foundation model that powers it. Foundation models are models trained on broad data at scale that can be adapted to various downstream tasks. Foundation models provide the base on which you can build various generative AI applications. Large language models (LLMs) are a subset of foundation models that are trained on a large corpus of text data.\nInterface and prompts To use a foundation model, you need an interface that provides access to it. The interface is generally an API that is managed, or it can be self-hosted using an open source or proprietary model. Self-hosting often involves procuring access to a machine learning environment that is supported by purpose-built accelerated computing instances to host the model. Using the API call, you can pass prompts to the foundation model and receive inference responses back.\nWorking with Datasets and Embeddings Although foundation models can generate human-like text, images, audio, and more from your prompts, this might not be sufficient for enterprise use cases. To power customized enterprise applications, the foundation models need relevant data from enterprise datasets.\nEnterprises accumulate huge volumes of internal data, such as documents, presentations, user manuals, reports, and transaction summaries, which the foundation model has never encountered. Ingesting and using enterprise data sources provide the foundation model with domain-specific knowledge to generate tailored, highly relevant outputs that align with the needs of the enterprise.\nYou can supply enterprise data to the foundation models as context along with the prompt, which will help the model to return more accurate outputs. How do you figure out the context to pass? For that, you need a way to search the enterprise datasets using the prompt text that is passed. This is where vector embeddings help.\nVector embeddings\nEmbedding is the process by which text, images, and audio are given numerical representation in a vector space. Embedding is usually performed by a machine learning model. The following diagram provides more details about embedding.\nEnterprise datasets, such as documents, images and audio, are passed to ML models as tokens and are vectorized. These vectors in an n-dimensional space, along with the metadata about them, are stored in purpose-built vector databases for faster retrieval.\nVector databases\nThe core function of vector databases is to compactly store billions of high-dimensional vectors representing words and entities. Vector databases provide ultra-fast similarity search across these billions of vectors in real time.\nThe most common algorithms used to perform the similarity search are k-nearest neighbors (k-NN) or cosine similarity.\nAmazon Web Services (AWS) offers the following as viable vector database options:\nAmazon OpenSearch Service (provisioned) Amazon OpenSearch Serverless pgvector extension in Amazon Relational Database Service (Amazon RDS) for PostgreSQL pgvector extension in Amazon Aurora PostgreSQL - Compatible Edition Note: For more information, refer to the following resources:\nK-Nearest Neighbor (k-NN) Search in Amazon OpenSearch Service Vector Engine for Amazon OpenSearch Serverless (Preview) Using PostgreSQL Extensions with Amazon RDS for PostgreSQL Amazon Aurora PostgreSQL Now Supports pgvector for Vector Storage and Similarity Search AWS also offers Pinecone in the AWS Marketplace, and there are open source, in-memory options, like Facebook AI Similarity Search (FAISS), Chroma, and many more.\nNote: For more information, refer to the following resources:\nPinecone FAISS Chroma Vectorized enterprise data\nAfter enterprise data is vectorized, you can search the given prompt in a vector database. You can supply the relevant chunks of information as context to improve the output of the generative AI model. This can reduce hallucinations, a phenomenon in which an LLM confidently generates plausible sounding but false information. Vector databases and context are used in Retrieval Augmented Generation (RAG).\nPrompt history store A prompt history store is another essential component in a generative AI application, particularly applications used for conversational AI, like chatbots. A prompt history store helps with contextually aware conversations that are both relevant and coherent. Many foundation models have a limited context window, which means you can only pass so much data as input to them. Storing state information in a multiple-turn conversation becomes difficult, which is why a prompt history store is needed. It can persist the state and make it possible to have long-term memory of the conversation.\nBy storing the history of prompts and responses, you can look up prompts from a previous conversation and avoid repetitive requests to the foundation model. This helps with requests from your audit and compliance teams about adherence to company policy and regulations. You can also debug prompt requests and responses to diagnose errors and warnings from your applications.\nFrontend web applications and mobile apps Often, you need to build a frontend application or app that acts as an interface for your users to use generative AI capabilities from the foundation model. The application or app is responsible for constructing prompts and calling the foundation model API. The responses from the foundation model are sanitized and filtered by the application or app before the users see them on their screens. The application or app should also handle failures and other unintended consequences in a seamless manner so the user experience is not affected.\n"
},
{
	"uri": "//localhost:1313/4-rags/",
	"title": "RAG Overview",
	"tags": [],
	"description": "",
	"content": "RAG is a framework for building generative AI applications that can make use of enterprise data sources and vector databases to overcome knowledge limitations. RAG works by using a retriever module to find relevant information from an external data store in response to a user\u0026rsquo;s prompt. This retrieved data is used as context, combined with the original prompt, to create an expanded prompt that is passed to the language model. The language model then generates a completion that incorporates the enterprise knowledge.\nWith RAG, language models can go beyond their original training data to use up-to-date, real-world information. RAG addresses the challenge of frequent data changes because it retrieves updated and relevant information instead of relying on potentially outdated sets of data.\nThe architecture diagram describes the components used in the RAG architecture and depicts the sequence of events. There are two distinct stages when using the RAG pattern. The lower portion of the diagram explains converting the existing knowledge documents into vector embeddings and storing them in a vector database. This phase is typically performed by a batch job. After it is complete, you can augment the user’s query with relevant information or documents using semantic search. You can then pass the user’s query and retrieved information into an LLM for completion.\nPhase 1\nThe focus of this phase is to convert the enterprise data used to augment input prompts into a compatible format to perform a relevancy search. This is done using an embeddings machine learning model. The batch job calls the model to convert existing knowledge documents into its numerical representations. The batch job then stores the data in a vector database using the approach described in the following three steps. To learn more, choose each of the following three numbered markers.\nPhase 2\nPhase 2 comprises seven steps. To learn more, choose each of the following seven numbered markers.\n"
},
{
	"uri": "//localhost:1313/5-bedrockfms/",
	"title": "Amazon Bedrock FMs",
	"tags": [],
	"description": "",
	"content": "Amazon Bedrock offers a wide choice of foundation models (FMs) from leading artificial intelligence (AI) startups and Amazon. Each of these FMs cater to different generative artificial intelligence (generative AI) use cases, such as summarization, language translation, coding, and image generation.\nNote: For users with screen readers, use table mode to read the table.\nCompany Foundation Model Description Amazon Amazon Titan Family of models built by Amazon that are pretrained on large datasets, which makes them powerfull, general-purpose models. AI21 Labs Jurassic-2 Multilingual large language models (LLMs) for text generation in Spanish, French, German, Portuguse, Italian, and Dutch. Anthropic Claude 2 LLM for thoughtful dialogue, content creation, complex reasioning, creativity, and coding based on Constitutional AI and harmlessness training. Cohere Command and Embed Text generation model for business applications and embeddings model for search, clustering, or classification in more than 100 languages. Now available: Stable Diffusion XL (SDXL) 1.0 Stability AI Stable Diffusion Text-to-image model for generation fo uinque, realistic, high-quality images, art, logos, and designs. Working with Amazon Bedrock FMs Some inference parameters are common across most models, such as temperature, Top P, Top K, and response length. You will dive deep into unique model-specific parameters and I/O configuration you can tune to achieve the desired output based on the use case.\nAmazon Titan\nAmazon Titan models are Amazon foundation models. Amazon offers the Amazon Titan Text model and the Amazon Titan Embeddings model through Amazon Bedrock.\nAmazon Titan models support the following unique inference parameters in addition to temperature, Top P, and response length, which are common parameters across multiple models.\nStop sequences\nWith stop sequences (stopSequences), you can specify character sequences to indicate where the model should stop. Use the pipe symbol (|) to separate different sequences (maximum 20 characters).\nAmazon Titan Text\nAmazon Titan Text is a generative LLM for tasks such as summarization, text generation, classification, open-ended question answering, and information extraction. The text generation model is trained on many different programming languages and Rich Text Format (RTF), like tables, JSON, comma-separated values (CSV), and others.\nInput { \u0026#34;inputText\u0026#34;: \u0026#34;\u0026lt;prompt\u0026gt;\u0026#34;, \u0026#34;textGenerationConfig\u0026#34; : { \u0026#34;maxTokenCount\u0026#34;: 512, \u0026#34;stopSequences\u0026#34;: [], \u0026#34;temperature\u0026#34;: 0.1, \u0026#34;topP\u0026#34;: 0.9 } } The following example shows the output from Amazon Titan Text for the input supplied in the previous code block. The model returns the output along with parameters, such as the number of input and output tokens generated.\nOutput { \u0026#34;inputTextTokenCount\u0026#34;: 613, \u0026#34;results\u0026#34;: [{ \u0026#34;tokenCount\u0026#34;: 219, \u0026#34;outputText\u0026#34;: \u0026#34;\u0026lt;output\u0026gt;\u0026#34; }] } Amazon Titan Embeddings\nThe Amazon Titan Embeddings model translates text inputs (words and phrases) into numerical representations (embeddings). Applications of this model include personalization and search. Comparing embeddings produces more relevant and contextual responses than word matching.\nInput { body = json.dumps({\u0026#34;inputText\u0026#34;: \u0026lt;prompt\u0026gt;,}) model_id = \u0026#39;amazon.titan-embed-text-v1\u0026#39; accept = \u0026#39;application/json\u0026#39; content_type = \u0026#39;application/json\u0026#39; response = bedrock_runtime.invoke_model( body=body, modelId=model_id, accept=accept, contentType=content_type ) response_body = json.loads(response[\u0026#39;body\u0026#39;].read()) embedding = response_body.get(\u0026#39;embedding\u0026#39;) } This will generate an embeddings vector consisting of numbers that look like the following output.\nOutput [0.82421875, -0.6953125, -0.115722656, 0.87890625, 0.05883789, -0.020385742, 0.32421875, -0.00078201294, -0.40234375, 0.44140625, ...] AI21 Jurassic-2 (Mid and Ultra) Common parameters for Jurassic-2 models include temperature, Top P, and stop sequences. Jurassic-2 models support the following unique parameters to control randomness, diversity, length, or repetition in the response:\nMax completion length (maxTokens): Specify the maximum number of tokens to use in the generated response. Presence penalty (presencePenalty): Use a higher value to lower the probability of generating new tokens that already appear at least once in the prompt or in the completion. Count penalty (countPenalty): Use a higher value to lower the probability of generating new tokens that already appear at least once in the prompt or in the completion. The value is proportional to the number of appearances. Frequency penalty (frequencyPenalty): Use a higher value to lower the probability of generating new tokens that already appear at least once in the prompt or in the completion. The value is proportional to the frequency of the token appearances (normalized to text length). Penalize special tokens: Reduce the probability of repetition of special characters. The default values are true as follows: Whitespaces (applyToWhitespaces): A true value applies the penalty to white spaces and new lines. Punctuations (applyToPunctuation): A true value applies the penalty to punctuation. Numbers (applyToNumbers): A true value applies the penalty to numbers. Stop words (applyToStopwords): A true value applies the penalty to stop words. Emojis (applyToEmojis): A true value excludes emojis from the penalty. Jurassic-2 Mid This is a mid-sized model that is optimized to follow natural language instructions and context, so there is no need to provide it with any examples. It is ideal for composing human-like text and solving complex language tasks, such as question answering, and summarization.\nJurassic-2 Ultra Ultra is a large-sized model that you can apply to language comprehension or generation tasks. Use cases include generating marketing copy, powering chatbots, assisting with creative writing, performing summarization, and extracting information.\nInput { \u0026#34;prompt\u0026#34;: \u0026#34;\u0026lt;prompt\u0026gt;\u0026#34;, \u0026#34;maxTokens\u0026#34;: 200, \u0026#34;temperature\u0026#34;: 0.5, \u0026#34;topP\u0026#34;: 0.5, \u0026#34;stopSequences\u0026#34;: [], \u0026#34;countPenalty\u0026#34;: {\u0026#34;scale\u0026#34;: 0}, \u0026#34;presencePenalty\u0026#34;: {\u0026#34;scale\u0026#34;: 0}, \u0026#34;frequencyPenalty\u0026#34;: {\u0026#34;scale\u0026#34;: 0} } Output { \u0026#34;id\u0026#34;: 1234, \u0026#34;prompt\u0026#34;: { \u0026#34;text\u0026#34;: \u0026#34;\u0026lt;prompt\u0026gt;\u0026#34;, \u0026#34;tokens\u0026#34;: [ { \u0026#34;generatedToken\u0026#34;: { \u0026#34;token\u0026#34;: \u0026#34;\\u2581who\\u2581is\u0026#34;, \u0026#34;logprob\u0026#34;: -12.980147361755371, \u0026#34;raw_logprob\u0026#34;: -12.980147361755371 }, \u0026#34;topTokens\u0026#34;: null, \u0026#34;textRange\u0026#34;: {\u0026#34;start\u0026#34;: 0, \u0026#34;end\u0026#34;: 6} }, //... ] }, \u0026#34;completions\u0026#34;: [ { \u0026#34;data\u0026#34;: { \u0026#34;text\u0026#34;: \u0026#34;\u0026lt;output\u0026gt;\u0026#34;, \u0026#34;tokens\u0026#34;: [ { \u0026#34;generatedToken\u0026#34;: { \u0026#34;token\u0026#34;: \u0026#34;\u0026lt;|newline|\u0026gt;\u0026#34;, \u0026#34;logprob\u0026#34;: 0.0, \u0026#34;raw_logprob\u0026#34;: -0.01293118204921484 }, \u0026#34;topTokens\u0026#34;: null, \u0026#34;textRange\u0026#34;: {\u0026#34;start\u0026#34;: 0, \u0026#34;end\u0026#34;: 1} }, //... ] }, \u0026#34;finishReason\u0026#34;: {\u0026#34;reason\u0026#34;: \u0026#34;endoftext\u0026#34;} } ] } Anthropic Claude 2 Anthropic Claude 2 is another model available for text generation on Amazon Bedrock. Claude is a generative AI model by Anthropic. It is purpose built for conversations, summarization, question answering, workflow automation, coding, and more. It supports everything from sophisticated dialogue and creative content generation to detailed instruction following.\nClaude uses common parameters, such as temperature, Top P, Top K, and stop sequences. In addition, Claude models use the following unique parameter to further tune the response output.\nMaximum length (max_tokens_to_sample): Specify the maximum number of tokens to use in the generated response. The following example shows an input configuration used to invoke a response from Anthropic Claude 2 using Amazon Bedrock.\nInput { \u0026#34;prompt\u0026#34;: \u0026#34;\\n\\nHuman:\u0026lt;prompt\u0026gt;\\n\\nAnswer:\u0026#34;, \u0026#34;max_tokens_to_sample\u0026#34;: 300, \u0026#34;temperature\u0026#34;: 0.5, \u0026#34;top_k\u0026#34;: 250, \u0026#34;top_p\u0026#34;: 1, \u0026#34;stop_sequences\u0026#34;: [\u0026#34;\\n\\nHuman:\u0026#34;] } Output { \u0026#34;completion\u0026#34;: \u0026#34;\u0026lt;output\u0026gt;\u0026#34;, \u0026#34;stop_reason\u0026#34;: \u0026#34;stop_sequence\u0026#34; } Cohere Command Command is the flagship text generation model by Cohere. It is trained to follow user commands and be useful instantly in practical business applications, such as summarization, copywriting, dialogue, extraction, and question answering. Optimized for business priorities, Cohere is System and Organizations Control (SOC) 2 compliant and emphasizes security, privacy, and responsible AI.\nIn addition to temperature, Top P, Top K, maximum length, and stop sequences, the Cohere Command model supports the following unique controls:\nReturn likelihoods (return_likelihoods): Specify how and if the token likelihoods are returned with the response. You can specify the following options: GENERATION: This option only returns likelihoods for generated tokens. ALL: This option returns likelihoods for all tokens. NONE: This option doesn’t return any likelihoods. This is the default option. Stream (stream): Specify true to return the response piece by piece in real time and false to return the complete response after the process finishes. The following example shows an input configuration used to invoke a response from Cohere Command using Amazon Bedrock.\nInput { \u0026#34;prompt\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;temperature\u0026#34;: float, \u0026#34;p\u0026#34;: float, \u0026#34;k\u0026#34;: float, \u0026#34;max_tokens\u0026#34;: int, \u0026#34;stop_sequences\u0026#34;: [\u0026#34;string\u0026#34;], \u0026#34;return_likelihoods\u0026#34;: \u0026#34;GENERATION|ALL|NONE\u0026#34;, \u0026#34;stream\u0026#34;: boolean, \u0026#34;num_generations\u0026#34;: int } Amazon Bedrock set up and configuration related APIs ListFoundationModels\nThis method is used to provide a list of Amazon Bedrock foundation models that you can use. The following example demonstrates how to list the base models using Python.\nInput %pip install --upgrade boto3 import boto3 import json bedrock = boto3.client(service_name=\u0026#39;bedrock\u0026#39;) model_list=bedrock.list_foundation_models() for x in range(len(model_list.get(\u0026#39;modelSummaries\u0026#39;))): print(model_list.get(\u0026#39;modelSummaries\u0026#39;)[x][\u0026#39;modelId\u0026#39;]) You get a list of all the foundation models available on Amazon Bedrock and their respective metadata information. Following is an example of the output filtered on the model ID.\nOutput amazon.titan-tg1-large amazon.titan-e1t-medium amazon.titan-embed-g1-text-02 amazon.titan-text-express-v1 amazon.titan-embed-text-v1 stability.stable-diffusion-xl stability.stable-diffusion-xl-v0 ai21.j2-grande-instruct ai21.j2-jumbo-instruct ai21.j2-mid ai21.j2-mid-v1 ai21.j2-ultra ai21.j2-ultra-v1 anthropic.claude-instant-v1 anthropic.claude-v1 anthropic.claude-v2 cohere.command-text-v14 Amazon Bedrock runtime-related APIs InvokeModel This API invokes the specified Amazon Bedrock model to run inference using the input provided in the request body. You use InvokeModel to run inference for text models, image models, and embedding models.\nRun inference on a text model You can send an invoke request to run inference on a model. Set the accept parameter to accept any content type in the response.\nThe following example details how to generate text with Python using the prompt \u0026ldquo;What is Amazon Bedrock?\u0026rdquo;\nInput bedrock_rt = boto3.client(service_name=\u0026#39;bedrock-runtime\u0026#39;) prompt = \u0026#34;What is Amazon Bedrock?\u0026#34; configs= { \u0026#34;inputText\u0026#34;: prompt, \u0026#34;textGenerationConfig\u0026#34;: { \u0026#34;maxTokenCount\u0026#34;: 4096, \u0026#34;stopSequences\u0026#34;: [], \u0026#34;temperature\u0026#34;:0, \u0026#34;topP\u0026#34;:1 } } body=json.dumps(configs) modelId = \u0026#39;amazon.titan-tg1-large\u0026#39; accept = \u0026#39;application/json\u0026#39; contentType = \u0026#39;application/json\u0026#39; response = bedrock_rt.invoke_model( body=body, modelId=modelId, accept=accept, contentType=contentType ) response_body = json.loads(response.get(\u0026#39;body\u0026#39;).read()) print(response_body.get(\u0026#39;results\u0026#39;)[0].get(\u0026#39;outputText\u0026#39;)) Sample output\nAmazon Bedrock is a managed service that makes foundation models from leading AI startups and Amazon Titan models available through APIs. For up-to-date information on Amazon Bedrock and how to use it, see the provided documentation and relevant FAQs.\nInvokeModelWithResponseStream This API invokes the specified Amazon Bedrock model to run inference using the input provided. It returns the response in a stream.\nFor streaming, you can set x-amzn-bedrock-accept-type in the header to contain the required content type of the response. In the following example, the header is set to accept any content type. The default value is application/json.\nThe example details how to generate streaming text with Python. It uses the Amazon titan-tg1-large model and the prompt \u0026ldquo;Write an essay for living on Mars using 10 sentences.\u0026rdquo;\nInput prompt = \u0026#34;Write an essay for living on Mars using 10 sentences.\u0026#34; configs= { \u0026#34;inputText\u0026#34;: prompt, \u0026#34;textGenerationConfig\u0026#34;: { \u0026#34;temperature\u0026#34;:0 } } body=json.dumps(configs) accept = \u0026#39;application/json\u0026#39; contentType = \u0026#39;application/json\u0026#39; modelId = \u0026#39;amazon.titan-tg1-large\u0026#39; response = bedrock_rt.invoke_model_with_response_stream( modelId=modelId, body=body, accept=accept, contentType=contentType ) stream = response.get(\u0026#39;body\u0026#39;) if stream: for event in stream: chunk = event.get(\u0026#39;chunk\u0026#39;) if chunk: print((json.loads(chunk.get(\u0026#39;bytes\u0026#39;).decode()))) Sample output\n{\u0026lsquo;outputText\u0026rsquo;: \u0026ldquo;\\nIt is difficult to imagine life on Mars because the planet is so different from Earth. The environment on Mars is extremely cold, dry, and dusty, making it difficult for organisms to survive without specialized adaptations. The planet\u0026rsquo;s low gravity also affec\u0026rdquo;, \u0026lsquo;index\u0026rsquo;: 0, \u0026rsquo;totalOutputTextTokenCount\u0026rsquo;: None, \u0026lsquo;completionReason\u0026rsquo;: None, \u0026lsquo;inputTextTokenCount\u0026rsquo;: 12} {\u0026lsquo;outputText\u0026rsquo;: \u0026rsquo;ts human physical and mental health, requiring special accommodations. However, with proper planning and preparation, it is possible for humans to live on Mars. To establish a sustainable colony on Mars, astronauts would need to live in specially designed habitats, wear protective gear, and rely on artificial systems for food, water, and air. Communication with Earth would be limited, and astronauts would need to be self-sufficient\u0026rsquo;, \u0026lsquo;index\u0026rsquo;: 0, \u0026rsquo;totalOutputTextTokenCount\u0026rsquo;: 128, \u0026lsquo;completionReason\u0026rsquo;: \u0026lsquo;LENGTH\u0026rsquo;, \u0026lsquo;inputTextTokenCount\u0026rsquo;: None}\nData Protection and Auditability Comprehensive data protection and privacy\nAmazon Bedrock provides comprehensive data protection and privacy. Your data used with Amazon Bedrock is not used for service improvement and is not shared with third-party model providers. You can use AWS PrivateLink with Amazon Bedrock to establish private connectivity between your FMs and your virtual private cloud (VPC) without exposing your traffic to the internet.\nYour data is encrypted in transit and at rest. You can customize FMs privately so you can control how your data is used and encrypted. Amazon Bedrock makes a separate copy of the base foundation model and trains the private copy of the model.\nSecure your generative AI applications\nTo secure your custom FMs, you can use AWS security services to form your defense in depth security strategy. Your customized FMs are encrypted using AWS Key Management Service (AWS KMS) keys, and they are stored encrypted.\nBy using AWS Identity and Access Management (IAM), you can control access to your customized FMs. You can allow or deny access to specific FMs, decide which services can get inferences, and choose who can log in to the Amazon Bedrock console.\nSupport for governance and auditability\nAmazon Bedrock offers comprehensive monitoring and logging capabilities and tools you can use to address your governance and audit requirements.\nYou can use Amazon CloudWatch to track usage metrics and build customized dashboards with metrics that you require for your audit purposes. Use AWS CloudTrail to monitor API activity and troubleshoot issues as you integrate other systems into your generative AI applications.\n"
},
{
	"uri": "//localhost:1313/6-langchain/",
	"title": "Optimizing LLM Performance",
	"tags": [],
	"description": "",
	"content": "LLM performance challenges Large language models (LLMs) are pretrained on large data collections, and they can perform multiple tasks, such as text generation, text summarization, question answering, and sentiment analysis. However, the models do not perform well when the task requires dealing with out-of-domain data or remembering conversational context.\nThese challenges lead to hallucinations or inaccurate responses. A single prompt to the LLM might not always provide the expected result. It might require the chaining of requests to the model to produce accurate results.\nSimplifying LLM development with LangChain LangChain provides the software building blocks to reduce the complexity of building functionality from scratch. You can use it to take full advantage of the power of the LLMs.\nLLMs do not retain state between invocations, so the application must manage any context. For example, to give a chat experience, the LLM needs the conversation as part of the context to produce the correct results. Additionally, LLMs can develop reasoning to solve problems, such as multistep problems where the application needs to find information in steps to solve a problem. LangChain provides components to make it more efficient to perform the common tasks of managing context or the sequencing of steps when interacting with an LLM.\nUsing LangChain components LangChain is open source and is currently available in three programming languages: Python, TypeScript, and JavaScript. LangChain consists of components such as schema, models, prompts, indexes, memory, chains, and agents.\nYou can use these components to build applications, such as the Retrieval Augmented Generation (RAG) based question answering chatbots. You can also use these components for text summarization, code generation, and interacting with APIs.\nSupported integrations for AWS LangChain supports integrations with numerous software providers, including Amazon. Some of the most important integrations related to Amazon include LLMs, prompt templates, chains, chat models, text embedding models, document loaders, retrievers, and agents.\nSchema The schema provides the structure for the conversation to construct the prompts generated by LangChain that are sent to the LLM. For example, the ChatMessages schema describes how to construct chat prompts with human messages, artificial intelligence (AI) messages, examples, and documents.\nIn the following lessons, you will learn about the most important AWS and LangChain integrations for models, prompt templates, indexes, memory, chains, and agents.\nLLMs\nLLMs take text as input and generate text as output, and LangChain provides LLM components to interact with different language models. The LLM class is an abstraction for working across different providers and is used by LangChain to interact with an LLM model.\nAmazon Bedrock\nLangChain supports Amazon Bedrock as an LLM. For more information, refer to the API. Amazon Bedrock currently supports Titan Text models from Amazon, Jurassic models from AI21, Claude from Anthropic, Cohere Command and Embed, Llama models from Meta, Stable Diffusion models from Stability AI, and Mistral models from Mistral AI.\nThe following example demonstrates how to create an instance of the Amazon Bedrock class and invoke an Amazon Titan LLM from the LangChain LLM module. The model_id is the value of the selected model available in Amazon Bedrock.\nimport boto3 from langchain_aws import BedrockLLM bedrock_client = boto3.client(\u0026#39;bedrock-runtime\u0026#39;,region_name=\u0026#34;us-east-1\u0026#34;) inference_modifiers = {\u0026#34;temperature\u0026#34;: 0.3, \u0026#34;maxTokenCount\u0026#34;: 512 llm = BedrockLLM( client = bedrock_client, model_id=\u0026#34;amazon.titan-tg1-large\u0026#34;, model_kwargs =inference_modifiers ) response = llm.invoke(\u0026#34;What is the largest city in Vermont?\u0026#34;) print(response) Chat models\nConversational interfaces, such as chatbots and virtual assistants, can lower the cost of customer support, while improving customer experience. LangChain provides a chat models component to build conversational applications. This component accepts content in the form of messages that contain input text.\nChat models example\nThe following example demonstrates how you can get a response from an LLM by passing a user request to the LLM.\nInput from langchain_aws import ChatBedrock as Bedrock from langchain.schema import HumanMessage chat = Bedrock(model_id=\u0026#34;anthropic.claude-3-sonnet-20240229-v1:0\u0026#34;, model_kwargs={\u0026#34;temperature\u0026#34;:0.1}) messages = [ HumanMessage( content=\u0026#34;I would like to try Indian food, what do you suggest should I try?\u0026#34; ) ] chat.invoke(messages) Sample response\nAIMessage(content=\u0026ldquo;Here are some delicious and popular Indian dish recommendations for someone trying Indian food for the first time:\\n\\n- Butter Chicken (Murgh Makhani) - Tender chicken in an aromatic tomato-based curry with butter and cream. It\u0026rsquo;s flavorful but not too spicy.\\n\\n- Chicken Tikka Masala - Chunks of roasted marinated chicken in a creamy tomato-based sauce.\\n\\n- Palak Paneer - A vegetarian dish with paneer (fresh cheese cubes) cooked in a thick spinach-based curry.\\n\\n- Chana Masala - A vegetarian chickpea curry flavored with spices like cumin, coriander and garam masala.\\n\\n- Samosas - Fried or baked pastry pockets stuffed with a savory potato/vegetable filling.\\n\\n- Naan - Leavened oven-baked flatbread, perfect for soaking up curries. Try garlic or butter naan.\\n\\n- Biryani - A flavorful mixed rice dish with meat/vegetables and aromatic spices.\\n\\nI\u0026rsquo;d recommend getting a combination platter or thali to sample a variety of dishes on your first visit. Start with milder dishes and work your way up if you want more heat/spice. Indian food has incredible depth of flavor!\u0026rdquo;, additional_kwargs={\u0026lsquo;usage\u0026rsquo;: {\u0026lsquo;prompt_tokens\u0026rsquo;: 23, \u0026lsquo;completion_tokens\u0026rsquo;: 300, \u0026rsquo;total_tokens\u0026rsquo;: 323}}, response_metadata={\u0026lsquo;model_id\u0026rsquo;: \u0026lsquo;anthropic.claude-3-sonnet-20240229-v1:0\u0026rsquo;, \u0026lsquo;usage\u0026rsquo;: {\u0026lsquo;prompt_tokens\u0026rsquo;: 23, \u0026lsquo;completion_tokens\u0026rsquo;: 300, \u0026rsquo;total_tokens\u0026rsquo;: 323}}, id=\u0026lsquo;run-09f0ef8a-90c0-47c3-b172-546b084d1288-0\u0026rsquo;)\nText embedding models\nText embedding models take text as input and then output numerical representations of the text in the form of a vector of floating-point numbers. The numerical representations are called word embeddings, and they capture the semantic meaning of the text.\nThe embeddings are used in various natural language processing (NLP) tasks, such as sentiment analysis, text classification, and information retrieval. You can save the embeddings in a vector database to improve search accuracy and for faster retrieval.\nEmbedding example\nThe following example demonstrates how to call a BedrockEmbeddings client to send text to the Amazon Titan Embeddings model to get embeddings as a response.\nfrom langchain.embeddings import BedrockEmbeddings embeddings = BedrockEmbeddings( region_name=\u0026#34;us-east-1\u0026#34;, model_id=\u0026#34;amazon.titan-embed-text-v1\u0026#34; ) embeddings.embed_query(\u0026#34;Cooper is a puppy that likes to eat beef\u0026#34;) In the following example, the baseline sentence is about a dog that likes to eat beef. A list of candidate phrases is compared against the baseline and sorted by similarity, with 0 being most similar and 1 being least similar.\nBaseline: Cooper is a dog that likes to eat beef.\nScore Text 0.035882 Cooper is a puppy that likes to eat beef. 0.083690 Beef is what a dog named Cooper likes. 0.109997 Cooper is a dog that hates eating beef. 0.135489 Cooper is a dog that hates eating beef. 0.184443 Cooper is a dog that likes to eat chicken 0.192230 Fido is a dog that likes to eat beef 0.194594 Cooper is a cat that likes to eat beef 0.228607 Spot is a dog that likes to eat beef. 0.264782 Cooper is a man that likes to eat beef 0.351404 Cooper ist ein Hund, der gerne Rindfleisch frisst. 0.799773 A cooper is someone that makes barrels. 1.016293 Amazon Web Services provides on-demand cloud computing solutions and APIs on a pay-as-you-go basis. Prompt templates in LangChain LangChain provides predefined prompt templates in the form of text strings that can take a set of parameters from the user and generate a prompt. Prompt templates make prompt engineering more efficient and make it possible to reuse prompts.\nPrompt example\nfrom langchain import PromptTemplate # Create a prompt template that has multiple input variables multi_var_prompt = PromptTemplate( input_variables=[\u0026#34;customerName\u0026#34;, \u0026#34;feedbackFromCustomer\u0026#34;], template=\u0026#34;\u0026#34;\u0026#34; Human: Create an email to {customerName} in response to the following customer service feedback that was received from the customer: \u0026lt;customer_feedback\u0026gt; {feedbackFromCustomer} \u0026lt;/customer_feedback\u0026gt; Assistant:\u0026#34;\u0026#34;\u0026#34; ) # Pass in values to the input variables prompt = multi_var_prompt.format(customerName=\u0026#34;John Doe\u0026#34;, feedbackFromCustomer=\u0026#34;\u0026#34;\u0026#34;Hello AnyCompany, I am very pleased with the recent experience I had when I called your customer support. I got an immediate call back, and the representative was very knowledgeable in fixing the problem. We are very happy with the response provided and will consider recommending it to other businesses. \u0026#34;\u0026#34;\u0026#34; ) Structuring Documents with Indexes Document loaders\nWhen building generative AI applications using the RAG approach, documents must be loaded from different sources to the LLMs to generate embeddings.\nLangChain provides the document loaders component, which is responsible for loading documents from various sources. Sources can include a database, an online store, or a local store. You can index and use information from these sources for information retrieval. You can use the document loaders to load different types of documents, such as HTML, PDF, and code.\nDocument loaders example\nThe following example demonstrates the loading of a document from Amazon Simple Storage Service (Amazon S3) using the S3FileLoader module.\nfrom langchain.document_loaders import S3FileLoader loader = S3FileLoader(\u0026#34;mysource_bucket\u0026#34;,\u0026#34;sample-file.docx\u0026#34;) data = loader.load() Retriever\nLangChain includes a retriever component for fetching relevant documents to combine with language models. When a user submits a query, the retriever searches through the document index to find the most relevant documents. It then sends the results to the application for further processing.\nWhen building RAG applications on AWS, you can use Amazon Kendra to index and query various data sources. Amazon Kendra is a fully managed service that provides semantic search capabilities for state-of-the-art ranking of documents and passages.\nAmazon Kendra also comes with pre-built connectors to popular data sources, such as Amazon S3, SharePoint, Confluence, and websites. It supports common document formats, such as HTML, Microsoft Word, PowerPoint, PDF, Excel, and PureText files.\nRetriever example\nThe following example demonstrates the use of the AmazonKendraRetriever to query an Amazon Kendra index and pass the results from that call to an LLM as context along with a prompt.\nfrom langchain.retrievers import AmazonKendraRetriever from langchain.chains import ConversationalRetrievalChain from langchain.prompts import PromptTemplate from langchain_aws import ChatBedrock llm = ChatBedrock( model_kwargs={\u0026#34;max_tokens_to_sample\u0026#34;:300,\u0026#34;temperature\u0026#34;:1,\u0026#34;top_k\u0026#34;:250,\u0026#34;top_p\u0026#34;:0.999,\u0026#34;anthropic_version\u0026#34;:\u0026#34;bedrock-2023-05-31\u0026#34;}, model_id=\u0026#34;anthropic.claude-3-sonnet-20240229-v1:0\u0026#34; ) retriever = AmazonKendraRetriever(index_id=kendra_index_id,top_k=5,region_name=region) prompt_template = \u0026#34;\u0026#34;\u0026#34; Human: This is a friendly conversation between a human and an AI. The AI is talkative and provides specific details from its context but limits it to 240 tokens. If the AI does not know the answer to a question, it truthfully says it does not know. Assistant: OK, got it, I\u0026#39;ll be a talkative truthful AI assistant. Human: Here are a few documents in \u0026lt;documents\u0026gt; tags: \u0026lt;documents\u0026gt; {context} \u0026lt;/documents\u0026gt; Based on the above documents, provide a detailed answer for, {question} Answer \u0026#34;do not know\u0026#34; if not present in the document. Assistant: \u0026#34;\u0026#34;\u0026#34; PROMPT = PromptTemplate( template=prompt_template, input_variables=[\u0026#34;context\u0026#34;, \u0026#34;question\u0026#34;] ) response = ConversationalRetrievalChain.from_llm( llm=llm, retriever=retriever, return_source_documents=True, combine_docs_chain_kwargs={\u0026#34;prompt\u0026#34;:PROMPT}, verbose=True) Vector stores\nWhen building generative AI applications, such as question answering bots, the LLMs produce accurate results when relevant company-specific or user-specific data is given as context.\nYou can use the RAG approach, and follow these steps:\nYou can convert the company-specific data, such as documents, into embeddings using the text embeddings model (described in the previous section). You can then store the embeddings in a vector database. You can extract the relevant documents based on the user request from the vector database. You can then pass them to the LLM as context for an accurate response. LangChain supports both open source and provider-specific vector stores. LangChain supports vector engine for Amazon OpenSearch Serverless and the pgvector extension available with Amazon Aurora PostgreSQL-Compatible Edition. LangChain provides the vector stores component to query the supported vector stores for relevant data.\nVector store example\nThe following example demonstrates how to create an instance of the VectorStore class and use OpenSearch Serverless as a vector store to query embeddings.\nimport os from langchain.embeddings import BedrockEmbeddings from langchain.vectorstores import OpenSearchVectorSearch index_name = os.environ[\u0026#34;AOSS_INDEX_NAME\u0026#34;] endpoint = os.environ[\u0026#34;AOSS_COLLECTION_ENDPOINT\u0026#34;] embeddings = BedrockEmbeddings(client=bedrock_client) vector_store = OpenSearchVectorSearch( index_name=index_name, embedding_function=embeddings, opensearch_url=endpoint, use_ssl=True, verify_certs=True, ) retriever = vector_store.as_retriever() LangChain memory LangChain memory provides the mechanism to store and summarize (if needed) prior conversational elements that are included in the context on subsequent invocations. LangChain provides components in the form of helper utilities for managing and manipulating previous chat messages. These utilities are modular. You can chain them with other components and interact with different types of abstractions to build powerful chatbots.\nIn LangChain, there are different ways you can implement conversational memory for a chatbot as follows:\nConversationBufferMemory: The ConversationBufferMemory is the most common type of memory in LangChain. It includes past conversations that happened between the user and the LLM.\nConversationChain: The ConversationBufferMemory is built on top of ConversationChain, which is designed for managing conversations. For a complete list of supported memory types\nMemory example\nThe following example demonstrates the use of ConversationBufferMemory to store the previous conversation and have the LLM respond to subsequent questions by using the chat history.\nfrom langchain.chains import ConversationChain from langchain_aws import BedrockLLM from langchain.memory import ConversationBufferMemory bedrock_client = boto3.client(\u0026#39;bedrock-runtime\u0026#39;,region_name=\u0026#34;us-east-1\u0026#34;) titan_llm = BedrockLLM(model_id=\u0026#34;amazon.titan-tg1-large\u0026#34;, client=bedrock_client) memory = ConversationBufferMemory() conversation = ConversationChain( llm=titan_llm, verbose=True, memory=memory ) print(conversation.predict(input=\u0026#34;hi! I am in Los Angeles. What are some of the popular site seeing places?\u0026#34;)) Ask a question without mentioning the city Los Angeles to find out how the model responds according to the previous conversation.\nprint(conversation.predict(input=\u0026#34;What is closest beach that I can go to?\u0026#34;)) Chaining components At its core, a chain is a set of components that run together. The component can be a call to an LLM, an API, or a sequence of other chains. The component has an input format and an output. For example, the LLMChain takes in an LLM, a prompt template, and parameters to the prompt template. It returns the output of the LLM call. The chain can parse the output of the LLM to a specific format and return the data in a more structured way.\nProcessing large amounts of data\nChains are also helpful when processing more data than can be contained in the context. For example, you might do a document summarization against a very large document. The chain will chunk up the document and make multiple calls to the LLM to produce a single summary.\nThere are different types of chains that LangChain supports, among which the LLMChain is a common one.\nChain example\nThe following example demonstrates how to use chains to call an LLM multiple times in a sequence.\nfrom langchain import PromptTemplate from langchain.chains import LLMChain from langchain_aws import ChatBedrock as Bedrock chat = Bedrock( region_name = \u0026#34;us-east-1\u0026#34;, model_kwargs={\u0026#34;temperature\u0026#34;:1,\u0026#34;top_k\u0026#34;:250,\u0026#34;top_p\u0026#34;:0.999,\u0026#34;anthropic_version\u0026#34;:\u0026#34;bedrock-2023-05-31\u0026#34;}, model_id=\u0026#34;anthropic.claude-3-sonnet-20240229-v1:0\u0026#34; ) multi_var_prompt = PromptTemplate( input_variables=[\u0026#34;company\u0026#34;], template=\u0026#34;Create a list with the names of the main metrics tracked in the reports of {company}?\u0026#34;, ) chain = LLMChain(llm=chat, prompt=multi_var_prompt) answers = chain.invoke(\u0026#34;Amazon\u0026#34;) print(answers) answers = chain.invoke(\u0026#34;AWS\u0026#34;) print(answers) Using LangChain agents LangChain agents can interact with external sources, such as search engines, calculators, APIs, or databases. The agents can also run code to perform actions to assist the LLMs in generating accurate responses. LangChain provides the chain interface to sequence multiple components to build an application. An LLMChain is an example of a basic chain.\nThe following mechanisms use basic chains:\nA RAG application can use an LLMChain to return a response. The response is based on two pieces of information: the user query and a context supplied as a set of documents retrieved from a vector store.\nA RouterChain is an example of a complex chain. For example, you can use a RouterChain to select one prompt template from the available prompt templates based on user input.\nThe LangChain agents act as reasoning engines for the LLMs to decide the actions to take and the order in which to take the actions. An action can be a tool that uses the results from a search engine or a math calculator.\nLangChain agent features\nThe LangChain agent follows a sequence of actions :\nLangChain provides tools and toolkits (a group of three to five different tools) in the form of functions for the agent to call. The agent is formed with access to an LLM, a set of tools, and a stopping condition. The agent uses the ReAct (reasoning and acting) prompting framework to choose the most relevant tool from the set of tools provided based on the user input. The agent repeatedly decides the action, runs the action, and observes the output of the tool until the stopping condition is met. Agent example\nThe following example demonstrates how to initialize an Agent, Tool, and LLM to form a chain and have the ZERO_SHOT ReAct agent call the in-built tool LLMMathChain to do math calculations separately and pass the result to the LLM for the final response.\nfrom langchain.agents import load_tools from langchain.agents import initialize_agent, Tool from langchain.agents import AgentType from langchain import LLMMathChain from langchain_aws import ChatBedrock from langchain.agents import AgentExecutor, create_react_agent chat = ChatBedrock(model_id=\u0026#34;anthropic.claude-3-sonnet-20240229-v1:0\u0026#34;, model_kwargs={\u0026#34;temperature\u0026#34;:0.1}) prompt_template = \u0026#34;\u0026#34;\u0026#34;Answer the following questions as best you can. You have access to the following tools:\\n\\n{tools}\\n\\n Use the following format:\\n\\nQuestion: the input question you must answer\\n Thought: you should always think about what to do\\n Action: the action to take, should be one of [{tool_names}]\\n Action Input: the input to the action\\nObservation: the result of the action\\n... (this Thought/Action/Action Input/Observation can repeat N times)\\n Thought: I now know the final answer\\n Final Answer: the final answer to the original input question\\n\\nBegin!\\n\\n Question: {input}\\nThought:{agent_scratchpad} \u0026#34;\u0026#34;\u0026#34; modelId = \u0026#34;anthropic.claude-3-sonnet-20240229-v1:0\u0026#34; react_agent_llm = ChatBedrock(model_id=modelId, client=bedrock_client) math_chain_llm = ChatBedrock(model_id=modelId, client=bedrock_client) tools = load_tools([], llm=react_agent_llm) llm_math_chain = LLMMathChain.from_llm(llm=math_chain_llm, verbose=True) llm_math_chain.llm_chain.prompt.template = \u0026#34;\u0026#34;\u0026#34;Human: Given a question with a math problem, provide only a single line mathematical expression that solves the problem in the following format. Don\u0026#39;t solve the expression only create a parsable expression. text {{single line mathematical expression that solves the problem}} Assistant: Here is an example response with a single line mathematical expression for solving a math problem: text 37593**(1/5) Human: {question} Assistant:\u0026#34;\u0026#34;\u0026#34; tools.append( Tool.from_function( func=llm_math_chain.run, name=\u0026#34;Calculator\u0026#34;, description=\u0026#34;Useful for when you need to answer questions about math.\u0026#34;, ) ) react_agent = create_react_agent(react_agent_llm, tools, PromptTemplate.from_template(prompt_template) # max_iteration=2, # return_intermediate_steps=True, # handle_parsing_errors=True, ) agent_executor = AgentExecutor( agent=react_agent, tools=tools, verbose=True, handle_parsing_errors=True, max_iterations = 10 # useful when agent is stuck in a loop ) agent_executor.invoke({\u0026#34;input\u0026#34;: \u0026#34;What is the distance between San Francisco and Los Angeles? If I travel from San Francisco to Los Angeles with the speed of 40MPH how long will it take to reach?\u0026#34;}) "
},
{
	"uri": "//localhost:1313/7-architecturepatterns/",
	"title": "Introduction to Architecture Patterns",
	"tags": [],
	"description": "",
	"content": "Architecture patterns In this module, you will learn about various architecture patterns that can be implemented with Amazon Bedrock for building useful generative AI applications such as the following:\nText generation Text summarization Question answering Chatbots Code generation LangChain agents Agents for Amazon Bedrock Text generation Text generation is a term used for any use case where the output of the model is newly generated text. You can use it to write articles, poems, blogs, books, emails, and so forth. In Amazon Bedrock, you can use various foundation models (FMs) for text generation tasks.\nThe architecture pattern for text generation using Amazon Bedrock is illustrated in the following image. You can pass the Amazon Bedrock foundation model a prompt using an Amazon Bedrock playground or an Amazon Bedrock API call. The model will generate text based on the input prompt you provide.\nText generation with LangChain\nFor text generation, you can also use a LangChain layer to add a conversation chain to specific text generation use cases. LangChain is a powerful open source library. It pairs well with some of the strongest text generation FMs on Amazon Bedrock to efficiently create conversations, text generation, and more. The architecture pattern for text generation with LangChain is illustrated in the following figure.\nText summarization Text summarization is a natural language processing (NLP) task that condenses the text from a given input while preserving the key information and meaning of the text. The following are the two ways to do summarization:\nSelect a subset of text from input that represents key ideas. Create new sentences that capture the key concepts and elements of the source text. With advances in generative AI and large language models (LLMs), the field of text summarization has witnessed significant improvements. LLMs are well suited for the task of generating summaries because of their effectiveness in understanding and synthesizing text. In this section, you will explore concepts, applications, and architecture patterns related to text summarization with large language models.\nQuestion answering architecture Question answering is an important task that involves extracting answers to factual queries posed in natural language. Typically, a question answering system processes a query against a knowledge base containing structured or unstructured data and generates a response with accurate information. Ensuring high accuracy is key to developing a useful, reliable, and trustworthy question answering system, especially for enterprise use cases.\nRAG involves the following steps:\nThe user asks a question. Domain-specific documents are converted into embeddings using the Amazon Titan Embeddings model. The embeddings are stored in a knowledge base (vector database) for subsequent retrieval. The user\u0026rsquo;s question is used to retrieve the relevant chunks of data, which will act as the context, from the knowledge base. The user\u0026rsquo;s question and the context are then passed to the FM to get an accurate response to the user. When the user poses a prompt, the FM identifies the context and refers to the knowledge base to get the relevant chunks of data. The FM then interacts with another FM to get an accurate response to the user. Chatbots Chatbots use NLP and machine learning (ML) algorithms to understand and respond to user queries. You can use chatbots in a variety of application, such as customer sevice, sales, and ecommerce, to provide quick and efficient responses to users. You can access chatbots through various channels, such as websites, social media platforms, and messaging apps.\nA basic architectural pattern of a chatbot use case with Amazon Bedrock is illustrated in the following diagram.\nThis architecture includes the following steps:\nThe user queries the chatbot. The chat history (if there is any) is passed on to the Amazon Bedrock model along with the user’s current query. The model then generates a response. The model passes the response back to the user. Chatbot use cases\nYou can categorize the chatbot use cases as follows:\nChatbot (Basic): This is a zero-shot chatbot with an FM model. Chatbot using a prompt template (LangChain): This is a chatbot with some context provided in the prompt template. Chatbot with a persona: This is a chatbot with defined roles, such as a career coach with human interactions. Contextual-aware chatbot: This is a chatbot that passes context through an external file by generating embeddings. Architecture for a context-aware chatbot\nA simple architecture for a context-aware chatbot is shown in the following diagram. This architecture includes the following steps:\nThe user asks a question (user query) to the LLM on Amazon Bedrock.\nStep 2:\na. The LLM sends the modified question to the embeddings model.\nb. The chat history is updated.\nc. The user query is converted to a vector embedding using the Amazon Titan Embeddings model.\nA similarity search is performed. The result of the search is a set of relevant text chunks.\nBased on the stored information, an answer to the prompt (user query) is generated from the final FM.\nStep 5:\na. The user query and the response from the FM are added to the chat history.\nb. The response (answer) is given to the user at the same time.\nCode Generation You can also use the foundation models in Amazon Bedrock for various coding and programming related tasks. Examples include code and SQL query generation, code explanation and translation, bug fixing, code optimization, and so forth. Using foundation models for coding related tasks helps developers and data scientists rapidly prototype their ideas and use cases.\nThe following architecture pattern illustrates the use case of using the FMs in Amazon Bedrock for coding and programming.\nThe steps are as follows:\nThe user enters a code prompt. A foundation model processes the input data. The model returns the generated code. LangChain agents Foundation models undergo extensive training on vast amounts of data. Despite their substantial natural language understanding capabilities, they cannot independently perform tasks like processing insurance claims or making flight reservations. This limitation arises from the necessity for access to the latest company or industry-specific data, which foundation models cannot obtain from up-to-date knowledge sources by default. Additionally, FMs cannot take specific actions to fulfill requests without a great deal of manual programming.\nCertain applications demand an adaptable sequence of calls to language models and various utilities depending on user input. The agent interface provides flexibility for these applications. An agent has availability to a range of resources and selects which ones to use based on the user input. Agents can use multiple tools, and they can use the output of one tool as the input for the next.\nThe architecture is illustrated in the following image.\nUsing ReAct to run agents on LangChain\nYou can run agents on LangChain by using one of two techniques: plan and execute or ReAct, which stands for reasoning and acting. The ReAct technique will evaluate the prompt and determine the next step in solving the problem. It will then run that step and then repeat the process until the LLM can answer the question. Plan and execute works a little differently in that it determines the steps needed ahead of time and performs them sequentially.\nExample: Connecting foundation models to your company data sources with agents for Amazon Bedrock\nUse RAG Agents for Amazon Bedrock use RAG to provide agents access to a knowledge base in Amazon Bedrock. The knowledge base is created in Amazon Bedrock and points to a data source on Amazon Simple Storage Service (Amazon S3) that contains your data. You have the option to sync your knowledge base in Amazon Bedrock to your data on Amazon S3. Select an embeddings model and vector database Next, you will select an embeddings model and provide details for a vector database. For the vector database, you can choose between Amazon OpenSearch Serverless, Pinecone, and Redis Enterprise Cloud. Add the knowledge base You can add the knowledge base when creating or updating agents for Amazon Bedrock. Create and add action groups When adding the knowledge base to the agent, you can also create and add action groups to it. Action groups are tasks that an agent can perform autonomously. You can provide Lambda functions that represent your business logic and the related API schema to run those functions. Although action groups are not required to create an agent, they can augment model performance to yield better outputs. "
},
{
	"uri": "//localhost:1313/8-getstart/",
	"title": "How to get started with Amazon Bedrock",
	"tags": [],
	"description": "",
	"content": "Amazon Bedrock tutorial Let’s look at a demo to get a better taste of Bedrock’s capabilities. To follow along, check out this GitHub repository: Amazon Bedrock – Introductory Demo.\nNote: Running this demo will incur costs on your AWS account. Prerequisites:\nAWS account (sandbox account recommended) IAM user or role with Administrator access or the required permissions to access Amazon Bedrock and its FMs. Configure this principal’s credentials in your environment’s default AWS profile (AWS_PROFILE). Also, ensure that you have enabled Model access on Amazon Bedrock. Python 3.9+ Internet access To get started, create a Python virtual environment:\npython -m venv demo cd demo source bin/activate Then, clone the repository and install the dependencies:\ngit clone https://github.com/aws-samples/amazon-bedrock-intro-demo.git cd amazon-bedrock-intro-demo pip install -r requirements.txt Finally, launch the streamlit application:\nstreamlit run main.py This command will open a new browser tab with the deployed application that looks like this:\nThere, you will find several applications and use cases calling the Bedrock APIs for different models. Let’s take a look at some of them.\nOn the left side panel, select QA – FM Comparison, where you can choose any of the available models and compare them for QA use cases. In this case, I am comparing the amazon.titan-tg1-large and anthropic.claude-v2:\nFeel free to play around and experiment with different models and different questions. Next, let’s check the Chat – FMs option. This time, I opted for the meta.llama2-13b-chat-v1 model and asked it to explain cloud computing like I am five years old.\nMoving to the Code Translation option allows us to translate code blocks from one programming language to another. I used a Javascript function that checks if a number is prime and opted to translate it to Python as an example:\nLastly, let’s explore the RAG – Document(s) option, which allows you to upload a PDF document and answer questions or retrieve information based on the document. It even shows you a comparison of the model’s answer with and without the RAG technique.\nAs for the document to use, I uploaded the European’s Parliament Artificial Intelligence Act and asked it to summarize the contents of the document. Note that the model is unaware of this document without using RAG, and its answer is entirely irrelevant. On the other hand, using the RAG method and leveraging the document, the answer was quite on point:\n"
},
{
	"uri": "//localhost:1313/categories/",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "//localhost:1313/tags/",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
}]